{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/title_card.png\" width=100% align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Why this project?\n",
    "\n",
    "As part of the Star Wars generation (or a [xennial](https://www.merriam-webster.com/words-at-play/words-were-watching-xennial)) I completely missed the Harry Potter craze in the early 2000s.  I saw the movies and the lines at bookstores, but wasn't that invested in the story myself.\n",
    "\n",
    "However, I do know lots of people who love the series, who consistently told me what a great, feminist character Hermione Granger is.  \n",
    "\n",
    "So, when my son started to get interested in <i>Harry Potter</i> I was excited to read it to him.  The series was very clearly centered around Harry but it had a major female character who is known for being smart and resourceful.  Sharing stories with major characters who do not look like my extremely white little boy has always been a priority for me as part of my mission to help him become a man who is thoughtful about the privileges he has and empathetic with others who do not share the same advantages.\n",
    "\n",
    "But, as I read the books for the first time with him as a bedtime story, I noticed something hadn't expected.  \n",
    "\n",
    "Subtle but prevalent sexism. \n",
    "\n",
    "Some of it was in how women and girls were characterized.  Hermione is a \"bossy know-it-all\", Aunt Petuntia has a \"shrill voice\".  Minerva McGonnigal is written a sharp, tight-lipped teacher while Mrs. Weasley is a nagging, frumpy housewife.  I found myself skipping over some passages because I didn't want my kid to hear about women being referred to as hags or cows with no repreive.\n",
    "\n",
    "At first I thought I was being overly sensitive.  Maybe I was just reading too much into these things?\n",
    "\n",
    "Channeling my inner Hermione, I did the equivalent of going to the library.  Turns out, I am not the first one to notice this.\n",
    "\n",
    "The sexism critique of the Harry Potter novels is not a new one - many people have (written)[https://www.bustle.com/articles/136244-the-5-least-feminist-moments-in-harry-potter] excellent articles about Ron's treatment of Hermoine, the portrayl of other female characters as cold or incompetent or promiscuous.  \n",
    "\n",
    "This post details the code behind my May 2018 [talk](http://codelandconf.com/speakers/eleanor-stribling/) of the same title for [Codeland](http://codelandconf.com/) in New York City.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the hypothesis & scope\n",
    "\n",
    "Because my talk was just 15 minutes, I needed to have a pretty specific scope for this project, at least this first iteration of it. I contrained the scope in four ways:\n",
    "- focus on the three protagonists rather than try to analyze every character; \n",
    "- analyze only the narrative, since it wasn't so much about how the characters speak, but how JK Rowling characterized women and girls that I had noticed to start with; \n",
    "- work with verbs and adverbs only - as they are used in describing every character pretty consistently throughout the books, adjectives less so and are harder to isolate; and\n",
    "- use \"out of the box\" tools as much as possible to keep the explanations straightforward and accessible in a 15 minute talk.\n",
    "\n",
    "\n",
    "The hypothesis I decided to explore:\n",
    "\n",
    "<b>Hermione will be described using terms associated with negative female characteristics that are not used for Harry and Ron.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaching the problem with code\n",
    "\n",
    "My approach boiled down into three steps:\n",
    "1. Prep the text for analysis in Python\n",
    "2. Isolate and group the parts of the text that need to be analyzed\n",
    "3. Perform the analysis\n",
    "\n",
    "\n",
    "A few tried and true tools were used for this project:\n",
    "- [Python](https://www.python.org/)\n",
    "- [Jupyter Notebook](http://jupyter.org/)\n",
    "- [Natural Language Processing Toolkit (NLTK)](http://www.nltk.org/)\n",
    "- [PyPlot](https://matplotlib.org/api/pyplot_api.html)\n",
    "\n",
    "The table below breaks down what the tools are and why I chose each of them:\n",
    "\n",
    "\n",
    "<img src =\"img/tools_table.png\" width=70%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Prep the text for analysis in Python\n",
    "\n",
    "The very first step is to get a copy of all of the books to analyze - I was able to get some .txt files, so that is reflected in the code below.\n",
    "\n",
    "No matter what format your files are in, however, the next step is the same: go from that file format to a string of characters that we can process with Python and NLTK.   This is done with the `read_file` function, below.\n",
    "\n",
    "To keep what comes next clear, we'll only read in just one book of the series (#1) unless otherwise stated.  In the way I've structured my code, this is indicated with the `num` variable that's passed in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(num):\n",
    "    text = ''\n",
    "    # In this code, the files are named 'hp' then the book number - i.e. `hp1.txt` - and stored in a folder \n",
    "    # called `corpus`. To run the code as it is, you'll need to recreate this schema or alter the code to \n",
    "    # fit the path you create.\n",
    "    with open('corpus/hp'+ str(num) + '.txt', 'rt') as file_in:\n",
    "        for line in file_in:\n",
    "            text = text + line\n",
    "    return text\n",
    "\n",
    "# here we create a variable called book_content and make it equal to the value of the read_file function for book 1\n",
    "book_content = read_file(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see what's in the book_content variable.  We'll work with this example through most of this post.\n",
      "\n",
      "\n",
      "Harry looked at Ron, and was relieved to see by his stunned face that he hadn't learned all the course books by heart either.\n",
      "\n",
      "\"I'm Ron Weasley,\" Ron muttered.\n",
      "\n",
      "\"Harry Potter,\" said Harry.\n",
      "\n",
      "\"Are you really?\" said Hermione.\n",
      "\n",
      "********************\n",
      "Looks good!  We can also double check the format of this variable is a string...\n",
      "<class 'str'>\n",
      "And how many characters it has...\n",
      "439676\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's see what's in the book_content variable.  We'll work with this example through most of this post.\")\n",
    "print(book_content[149756:149980])\n",
    "print()\n",
    "print('*' * 20)\n",
    "print(\"Looks good!  We can also double check the format of this variable is a string...\")\n",
    "print(type(book_content))\n",
    "print(\"And how many characters it has...\")\n",
    "print(len(book_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've read the text in and we have a giant string, we need to tokenize the text.\n",
    "\n",
    "Tokenizing might sound fancy, but what it really means is turning a long string into a list of words; it's also called splitting the string on the spaces.  For example:\n",
    "\n",
    "`\"Are you really?\" said Hermione. ` from our `book_content` variable above would become:\n",
    "\n",
    "`['\"', 'Are', 'you', 'really', '?', '\"', 'said', 'Hermione', '.']`\n",
    "\n",
    "We'll import a function of NLTK called `word_tokenize` and write a function called `tokenize_lower_text` around it.  We'll pass in our `book_content` variable, the big string of every character from Book 1.  In this function, we'll also put every word into lower case, since computers are very literal and will interpret `Hermione` and `hermione` as two different words.\n",
    "\n",
    "In most text processing projects you'd also remove punctuation and maybe stop words (aka common words).  We need the punctuation (you'll see why in a moment) and the stop words don't hurt our analysis, so I've left them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize_lower_text(book_content):\n",
    "    tokenized = word_tokenize(book_content)\n",
    "    tokenized = [word.lower() for word in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "tokenized = tokenize_lower_text(book_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['harry', 'looked', 'at', 'ron', ',', 'and', 'was', 'relieved', 'to', 'see', 'by', 'his', 'stunned', 'face', 'that', 'he', 'had', \"n't\", 'learned', 'all', 'the', 'course', 'books', 'by', 'heart', 'either', '.', '``', 'i', \"'m\", 'ron', 'weasley', ',', \"''\", 'ron', 'muttered', '.', '``', 'harry', 'potter', ',', \"''\", 'said', 'harry', '.', '``', 'are', 'you', 'really', '?', \"''\", 'said', 'hermione', '.']\n"
     ]
    }
   ],
   "source": [
    "# Let's check our output - we should see a list of lower case words, same text as before.\n",
    "print(tokenized[33746:33800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for step 1!  Now that we've gotten the text ready for processing, we need to seperate out the parts we need and group them to prep for the analysis.  Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Isolate and group the parts of the text that need to be analyzed\n",
    "\n",
    "Now the real fun begins!  \n",
    "\n",
    "Remember how we want to seperate dialog from narration?  Here's where our punctuation gets important.\n",
    "\n",
    "As you can see in the text above, our opening and closing quotes look like this: `'``'` and `\"''\"`. This is a really helpful pattern when it comes to splitting dialog from narrative.  \n",
    "\n",
    "That's exactly what the `parse_text` function below does, and returns a list of dialog and a list of narrative.  It will:\n",
    "- take our `tokenized` variable as an input\n",
    "- loop through that list of words\n",
    "   - if the word is not a quotation mark, append it to a list called `current`\n",
    "   - if the word is an open quote (e.g. `'``'` which is the value of the `open_q` variable in the code below):\n",
    "      - append the contents of `current` to the `parsed_narrative` list, clear `current`\n",
    "      - change the value of the found_q variable from `False` to `True`\n",
    "      - while `found_q` is True, append each word to the `current` list\n",
    "        - when you hit a close_q (closing quote):\n",
    "        - append it to `current`\n",
    "        - append `current` to `parsed_dialog`\n",
    "        - clear the `current` list\n",
    "        - switch the `found_q` variable to false, which gets us out of the while loop and into the case where the text will be added to the narration variable until we get to a quotation mark again.\n",
    "        \n",
    "This will return two lists, one of dialog, one of narration, that will contain lists of each passage of tokenized dialog or narrative.  These individual passages are each a list of their own, but can contain multiple sentences.  For example, the beginning of book one has a pretty big chunk of narration before the dialog starts, and all of that would be one item in the list.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(t):\n",
    "    open_q = '``'\n",
    "    close_q = \"''\"\n",
    "    found_q = False # this will be used to break the while loop below\n",
    "    # current will hold words until an open quote is found\n",
    "    current = []\n",
    "    \n",
    "    parsed_dialog = [] \n",
    "    parsed_narrative = []\n",
    "    length = len(t)\n",
    "    i = 0\n",
    "\n",
    "    while i < length:\n",
    "        word = t[i]\n",
    "        \n",
    "        if word != open_q and word != close_q:\n",
    "            current.append(word)\n",
    "\n",
    "        elif word == open_q or word == close_q:\n",
    "            parsed_narrative.append(current)\n",
    "\n",
    "            current = []\n",
    "            current.append(word)\n",
    "\n",
    "            while found_q == False and i < length-1:\n",
    "                i += 1\n",
    "                if t[i] != close_q:\n",
    "                    current.append(t[i])\n",
    "                else:\n",
    "                    current.append(t[i])\n",
    "                    parsed_dialog.append(current)\n",
    "                    current = []\n",
    "                    found_q = True\n",
    "        \n",
    "        found_q = False\n",
    "        i += 1\n",
    "        \n",
    "    return (parsed_dialog, parsed_narrative)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_narrative: [['she', 'said', 'all', 'this', 'very', 'fast', '.', 'harry', 'looked', 'at', 'ron', ',', 'and', 'was', 'relieved', 'to', 'see', 'by', 'his', 'stunned', 'face', 'that', 'he', 'had', \"n't\", 'learned', 'all', 'the', 'course', 'books', 'by', 'heart', 'either', '.'], ['ron', 'muttered', '.'], ['said', 'harry', '.'], ['said', 'hermione', '.']]\n",
      "\n",
      "parsed_dialog: [['``', 'i', \"'m\", 'ron', 'weasley', ',', \"''\"], ['``', 'harry', 'potter', ',', \"''\"], ['``', 'are', 'you', 'really', '?', \"''\"]]\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at what our original selection looks like now.\n",
    "print(\"parsed_narrative:\", parsed_narrative[711:715])\n",
    "print()\n",
    "print(\"parsed_dialog:\",parsed_dialog[711:714])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our narration isolated in the `parsed_narrative` list.  We won't need the `parsed_dialog` list for this project.  \n",
    "\n",
    "Now that we have this list, we need to split the longer passages into single sentences.  This will make analyzing the text a little easier. Let's look at an example of how long these passages of narration can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of narrative list: ['chapter', 'one', 'the', 'boy', 'who', 'lived', 'mr.', 'and', 'mrs.', 'dursley', ',', 'of', 'number', 'four', ',', 'privet', 'drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'they', 'were', 'the', 'last', 'people', 'you', \"'d\", 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'did', \"n't\", 'hold', 'with', 'such', 'nonsense', '.', 'mr.', 'dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'grunnings', ',', 'which', 'made', 'drills', '.', 'he', 'was', 'a', 'big', ',', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', ',', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', '.', 'mrs.', 'dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', ',', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'craning', 'over', 'garden', 'fences', ',', 'spying', 'on', 'the', 'neighbors', '.', 'the', 'dursleys', 'had', 'a', 'small', 'son', 'called', 'dudley', 'and', 'in', 'their', 'opinion', 'there', 'was', 'no', 'finer', 'boy', 'anywhere', '.', 'the', 'dursleys', 'had', 'everything', 'they', 'wanted', ',', 'but', 'they', 'also', 'had', 'a', 'secret', ',', 'and', 'their', 'greatest', 'fear', 'was', 'that', 'somebody', 'would', 'discover', 'it', '.', 'they', 'did', \"n't\", 'think', 'they', 'could', 'bear', 'it', 'if', 'anyone', 'found', 'out', 'about', 'the', 'potters', '.', 'mrs.', 'potter', 'was', 'mrs.', 'dursley', \"'s\", 'sister', ',', 'but', 'they', 'had', \"n't\", 'met', 'for', 'several', 'years', ';', 'in', 'fact', ',', 'mrs.', 'dursley', 'pretended', 'she', 'did', \"n't\", 'have', 'a', 'sister', ',', 'because', 'her', 'sister', 'and', 'her', 'good-for-nothing', 'husband', 'were', 'as', 'undursleyish', 'as', 'it', 'was', 'possible', 'to', 'be', '.', 'the', 'dursleys', 'shuddered', 'to', 'think', 'what', 'the', 'neighbors', 'would', 'say', 'if', 'the', 'potters', 'arrived', 'in', 'the', 'street', '.', 'the', 'dursleys', 'knew', 'that', 'the', 'potters', 'had', 'a', 'small', 'son', ',', 'too', ',', 'but', 'they', 'had', 'never', 'even', 'seen', 'him', '.', 'this', 'boy', 'was', 'another', 'good', 'reason', 'for', 'keeping', 'the', 'potters', 'away', ';', 'they', 'did', \"n't\", 'want', 'dudley', 'mixing', 'with', 'a', 'child', 'like', 'that', '.', 'when', 'mr.', 'and', 'mrs.', 'dursley', 'woke', 'up', 'on', 'the', 'dull', ',', 'gray', 'tuesday', 'our', 'story', 'starts', ',', 'there', 'was', 'nothing', 'about', 'the', 'cloudy', 'sky', 'outside', 'to', 'suggest', 'that', 'strange', 'and', 'mysterious', 'things', 'would', 'soon', 'be', 'happening', 'all', 'over', 'the', 'country', '.', 'mr.', 'dursley', 'hummed', 'as', 'he', 'picked', 'out', 'his', 'most', 'boring', 'tie', 'for', 'work', ',', 'and', 'mrs.', 'dursley', 'gossiped', 'away', 'happily', 'as', 'she', 'wrestled', 'a', 'screaming', 'dudley', 'into', 'his', 'high', 'chair', '.', 'none', 'of', 'them', 'noticed', 'a', 'large', ',', 'tawny', 'owl', 'flutter', 'past', 'the', 'window', '.', 'at', 'half', 'past', 'eight', ',', 'mr.', 'dursley', 'picked', 'up', 'his', 'briefcase', ',', 'pecked', 'mrs.', 'dursley', 'on', 'the', 'cheek', ',', 'and', 'tried', 'to', 'kiss', 'dudley', 'good-bye', 'but', 'missed', ',', 'because', 'dudley', 'was', 'now', 'having', 'a', 'tantrum', 'and', 'throwing', 'his', 'cereal', 'at', 'the', 'walls', '.']\n"
     ]
    }
   ],
   "source": [
    "parsed_dialog, parsed_narrative = parse_text(tokenized)\n",
    "\n",
    "print(\"Sample of narrative list:\", parsed_narrative[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split_sent` is a helper function (below) that will take the `parsed_narrative` list and split it into sentences. \n",
    "\n",
    "We also want to group the sentences by protagonist.  Because prepositions (he, she, they, etc) are difficult to attribute to a noun with certainty in a programmatic way, I stuck with sentences where protagonists were referred to by name.  The function called `protagonist` below creates a dictionary where the keys are the names of the protagonists, and the values are the tokenized sentences that mention them.   We'll discard all of the other narrative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby as gb\n",
    "\n",
    "p_list = ['harry', 'ron', 'hermione']\n",
    "\n",
    "def split_sent(t):\n",
    "    sent_list = []\n",
    "    for sent in t:\n",
    "        k = [list(sent) for i, sent in gb(sent, lambda item: item=='.')]\n",
    "        for i in k:\n",
    "            if len(i) > 1:\n",
    "                sent_list.append(i)\n",
    "    return (sent_list)\n",
    "\n",
    "def protagonist(n, p_list):\n",
    "    protagonist_narrative = {}\n",
    "    for p in p_list:\n",
    "        protagonist_narrative[p] = []\n",
    "\n",
    "    for i in n:\n",
    "        for word in i:\n",
    "            \n",
    "            if word in p_list:\n",
    "                if word == p_list[0]:\n",
    "                    protagonist_narrative['harry'].append(i)\n",
    "                if word == p_list[1] :\n",
    "                    protagonist_narrative['ron'].append(i)\n",
    "                if word == p_list[2]:\n",
    "                    protagonist_narrative['hermione'].append(i)\n",
    "                    \n",
    "    return protagonist_narrative\n",
    "        \n",
    "\n",
    "narrative_split_sent = split_sent(parsed_narrative)\n",
    "protagonist_dict = protagonist(narrative_split_sent, p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from Harry key:\n",
      "[['he', 'was', 'sure', 'there', 'were', 'lots', 'of', 'people', 'called', 'potter', 'who', 'had', 'a', 'son', 'called', 'harry'], ['come', 'to', 'think', 'of', 'it', ',', 'he', 'was', \"n't\", 'even', 'sure', 'his', 'nephew', 'was', 'called', 'harry'], ['she', 'eyed', 'his', 'cloak', 'suddenly', 'as', 'though', 'she', 'thought', 'he', 'might', 'be', 'hiding', 'harry', 'underneath', 'it'], ['dumbledore', 'took', 'harry', 'in', 'his', 'arms', 'and', 'turned', 'toward', 'the', 'dursleys', \"'\", 'house'], ['he', 'bent', 'his', 'great', ',', 'shaggy', 'head', 'over', 'harry', 'and', 'gave', 'him', 'what', 'must', 'have', 'been', 'a', 'very', 'scratchy', ',', 'whiskery', 'kiss']]\n",
      "Sample from Ron key:\n",
      "[['said', 'ron'], ['said', 'ron', 'again'], ['mumbled', 'ron'], ['said', 'harry', 'and', 'ron'], ['ron', 'blurted', 'out']]\n",
      "Sample from Hermione key:\n",
      "[['said', 'hermione'], ['said', 'hermione'], ['he', 'was', 'just', 'taking', 'harry', 'through', 'the', 'finer', 'points', 'of', 'the', 'game', 'when', 'the', 'compartment', 'door', 'slid', 'open', 'yet', 'again', ',', 'but', 'it', 'was', \"n't\", 'neville', 'the', 'toadless', 'boy', ',', 'or', 'hermione', 'granger', 'this', 'time'], ['perhaps', 'they', 'thought', 'there', 'were', 'more', 'rats', 'lurking', 'among', 'the', 'sweets', ',', 'or', 'perhaps', 'they', \"'d\", 'heard', 'footsteps', ',', 'because', 'a', 'second', 'later', ',', 'hermione', 'granger', 'had', 'come', 'in'], ['he', 'turned', 'to', 'hermione']]\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first few entries from the  key to see what it looks like.\n",
    "for p in p_list:\n",
    "        print(\"Sample from %s key:\" % p.title())\n",
    "        print(protagonist_dict[p][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Perform the analysis\n",
    "Now that we've got all of the narrative passages mentioning the protagonists organized, we can use NLTK's classification function to categorize words by the part of speech they represent.\n",
    "\n",
    "We'll use the `pos_tag` function of NLTK to hone in on the verbs and adjectives in each sentence.  The `pos_tag` function takes in a tokenized word and returns a tuple of the word and a code for how it was classified.  For example, 'NNP' means the word has been tagged by NLTK as a proper noun, 'VB' means the word has been tagged as a verb.  So \"Harry\" in the sentence \"Harry is Petunia's nephew.\" would come back as `('Harry', 'NNP')`.  \n",
    "\n",
    "You can find a full list [here](https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/), but in the next steps we'll be focused primarily on adjectives, verbs and nouns.\n",
    "\n",
    "We will import the `pos_tag` function from NLTK and tag the words in a new helper function called `tagged_text` that returns an updated list.\n",
    "\n",
    "We'll call this function inside another one called `parse_tagged` that will return a dictionary with keys for each protagonist followed by lists of tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def tagged_text(i):\n",
    "    tagged = [pos_tag(word) for word in i]\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry : \n",
      "[[('he', 'PRP'), ('was', 'VBD'), ('sure', 'RB'), ('there', 'EX'), ('were', 'VBD'), ('lots', 'NNS'), ('of', 'IN'), ('people', 'NNS'), ('called', 'VBN'), ('potter', 'NN'), ('who', 'WP'), ('had', 'VBD'), ('a', 'DT'), ('son', 'NN'), ('called', 'VBN'), ('harry', 'NN')], [('come', 'VB'), ('to', 'TO'), ('think', 'VB'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('he', 'PRP'), ('was', 'VBD'), (\"n't\", 'RB'), ('even', 'RB'), ('sure', 'JJ'), ('his', 'PRP$'), ('nephew', 'NN'), ('was', 'VBD'), ('called', 'VBN'), ('harry', 'NN')]]\n",
      "Ron : \n",
      "[[('said', 'VBD'), ('ron', 'NN')], [('said', 'VBD'), ('ron', 'NN'), ('again', 'RB')]]\n",
      "Hermione : \n",
      "[[('said', 'VBD'), ('hermione', 'NN')], [('said', 'VBD'), ('hermione', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "def parse_tagged(protagonist_dict):\n",
    "    tagged_dict = {}\n",
    "    \n",
    "    for k, v in protagonist_dict.items():\n",
    "        tagged_dict[k] = tagged_text(v)\n",
    "    return tagged_dict\n",
    "    \n",
    "# Call the function and save the results in a variable called tagged_dict.\n",
    "tagged_dict = parse_tagged(protagonist_dict)\n",
    "\n",
    "# Let's see what these look like by printing the first two sentences in each key.\n",
    "for p in p_list:\n",
    "        print(\"%s : \" % p.title())\n",
    "        print(tagged_dict[p][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tagged_dict` contains all of the words in every sentence involving each protagonist.  The function below, `descriptor_verbs_adverbs` is going to pull out only verbs and adverbs.  However, we can't just pull out every verb and adverb in a sentence that matches the protagonist's name - we won't know for sure that the words are being used to describe them.\n",
    "\n",
    "So, the function below seeks out specific patterns of verbs and adverbs around the protagonist's name that we know will be referring to them.  \n",
    "\n",
    "These are:\n",
    "<img src=\"img/verb_patterns.png\" width=80%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harry\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "ron\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "hermione\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "yo\n",
      "['said', 'said', 'said', 'heard', 'gave', 'gave', 'ignored', 'ignoring', 'shouted', 'was', 'was', 'heard', 'put', 'was', 'wheedled', 'said', 'cried', 'said', 'said', 'said', 'said', 'said', 'said', 'said', 'told', 'said', 'suddenly', 'shrieked', 'said', 'thoughtfully', 'said', 'said', 'said', 'said', 'found', 'said', 'told', 'expected', 'said', 'faintly', 'asked', 'whispered', 'said', 'panted', 'said', 'said', 'said', 'said', 'said', 'said', 'said', 'said', 'shrieked', 'said', 'said', 'said', 'said', 'nervously', 'said', 'said', 'said', 'said', 'said', 'said']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def descriptor_verbs_adverbs(td):\n",
    "    descriptor_verbs = {}\n",
    "    \n",
    "    for k, v in td.items():\n",
    "        # Create a key in the dictionary for the protagonist.\n",
    "        descriptor_verbs[k] = []\n",
    "        print(k)\n",
    "        # Instaniate a counter object - this will help us count up numbers of times these words appear.\n",
    "        cnt = Counter()\n",
    "        \n",
    "        # Loop through each sentence in the list\n",
    "        for s in v:\n",
    "            for i in range(len(s)):\n",
    "                \n",
    "                if 'VB' in s[i][1]: \n",
    "                    try:\n",
    "                        # Cases of NOUN - VERB (e.g. \"said Ron\")\n",
    "                        if s[i+1][0] == k:\n",
    "                            descriptor_verbs[k].append(s[i][0])\n",
    "                            \n",
    "                            # Cases where NOUN - VERB is followed by an ADVERB (e.g. \"said Ron angrily\")\n",
    "                            try:\n",
    "                                if 'RB' in s[i+2][1]:\n",
    "                                    descriptor_verbs[k].append(s[i+2][0])\n",
    "                            except:\n",
    "                                continue\n",
    "                            \n",
    "                    except:\n",
    "                        continue\n",
    "#                 try:\n",
    "#                     # cases of VERB-NOUN-ADVERB e.g. \"said Ron\"\n",
    "#                     if 'VB' in s[i][1] and s[i+1][0] == k:\n",
    "#                         print('yes')\n",
    "#                         try: \n",
    "#                             # cases of VERB-NOUN-ADVERB e.g. \"said Ron angrily\"\n",
    "#                             if s[i+2][1] and s[i+2][1] == 'RB':\n",
    "#                                 print(\"adverb\")\n",
    "#                         except:\n",
    "#                             print(\"no adverb\")\n",
    "                    \n",
    "                    \n",
    "#                     and s[i+1][1]:\n",
    "#                         if s[i+2][1] == 'RB':\n",
    "#                             cnt[s[i][0]] += 1\n",
    "#                             cnt[s[i+2][0]] += 1\n",
    "#                             if k == 'Hermione':\n",
    "#                                 hermione.append(s[i+2][0])\n",
    "#                                 hermione.append(s[i][0])\n",
    "#                             if k == 'Harry':\n",
    "#                                 harry.append(s[i+2][0])\n",
    "#                                 harry.append(s[i][0])\n",
    "#                             if k == 'Ron':\n",
    "#                                 ron.append(s[i][0])\n",
    "#                                 ron.append(s[i+2][0])\n",
    "#                 except:\n",
    "#                     pass\n",
    "                \n",
    "#                 try:\n",
    "#                     # cases of NOUN-VERB-ADVERB e.g. \"Ron said angrily\"\n",
    "#                     if s[i][0] == k and 'VB' in s[i+1][1]:\n",
    "#                         cnt[s[i+1][0]] += 1\n",
    "#                         try:\n",
    "#                             if s[i+2][1] =='RB':\n",
    "#                                 cnt[s[i+1][0]] += 1\n",
    "#                                 cnt[s[i+2][0]] += 1\n",
    "#                                 if k == 'Hermione':\n",
    "#                                     hermione.append(s[i+1][0])\n",
    "#                                     hermione.append(s[i+2][0])\n",
    "#                                 if k == 'Harry':\n",
    "#                                     harry.append(s[i+1][0])\n",
    "#                                     harry.append(s[i+2][0])\n",
    "#                                 if r == 'Ron':\n",
    "#                                     ron.append(s[i+1][0])\n",
    "#                                     ron.append(s[i+2][0])\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                 except:\n",
    "#                     pass\n",
    "        \n",
    "#         speech_descriptors[k] = cnt\n",
    "        \n",
    "    return (descriptor_verbs)\n",
    "\n",
    "descriptor_verbs = descriptor_verbs_adverbs(tagged_dict)\n",
    "print(descriptor_verbs['hermione'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Harry ********************\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-90c55442312b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#harry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Harry\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mharry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/potter/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \"\"\"\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/potter/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \"\"\"\n\u001b[1;32m    552\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/potter/lib/python3.6/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0;32m--> 351\u001b[0;31m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "#generate the word cloud with parameters\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", \n",
    "               max_words=50, \n",
    "               min_font_size =5, \n",
    "               max_font_size=40, \n",
    "               relative_scaling = 0.4, \n",
    "               normalize_plurals= True)\n",
    "\n",
    "fig_sz = (20,20)\n",
    "\n",
    "#harry\n",
    "print (\"*\" * 20, \"Harry\", \"*\" * 20)\n",
    "wc.generate(' '.join(harry))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#ron\n",
    "print (\"*\" * 20, \"Ron\", \"*\" * 20)\n",
    "wc.generate(' '.join(ron))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Show the wordcloud\n",
    "plt.show()\n",
    "\n",
    "#hermione\n",
    "print (\"*\" * 20, \"Hermione\", \"*\" * 20)\n",
    "wc.generate(' '.join(hermione))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Show the wordcloud\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_exclusive(char, speech, p_list):\n",
    "    char_list = speech[char].keys()\n",
    "    other_chars = []\n",
    "    \n",
    "    for item in p_list:\n",
    "        if item != char and item != \"Ronald\":\n",
    "            other_chars.extend(speech[item])\n",
    "    \n",
    "    common_words = set(other_chars)\n",
    "    exclusive_words = []\n",
    "    \n",
    "    for w in char_list:\n",
    "        if w not in common_words:\n",
    "            for n in range(speech[char][w]):\n",
    "                exclusive_words.append(w)\n",
    "    \n",
    "    return exclusive_words\n",
    "    \n",
    "    \n",
    "hermione_excl = character_exclusive('Hermione', speech, p_list)\n",
    "harry_excl = character_exclusive('Harry', speech, p_list)\n",
    "ron_excl = character_exclusive('Ron', speech, p_list)\n",
    "\n",
    "print(Counter(harry_excl).most_common(10))\n",
    "print(Counter(ron_excl).most_common(10))\n",
    "print(Counter(hermione_excl).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#harry only\n",
    "print (\"*\" * 20, \"Harry\", \"*\" * 20)\n",
    "wc.generate(' '.join(harry_excl))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#ron only\n",
    "print (\"*\" * 20, \"Ron\", \"*\" * 20)\n",
    "wc.generate(' '.join(ron_excl))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Show the wordcloud\n",
    "plt.show()\n",
    "\n",
    "#hermione\n",
    "print (\"*\" * 20, \"Hermione\", \"*\" * 20)\n",
    "wc.generate(' '.join(hermione_excl))\n",
    "plt.figure(figsize=fig_sz)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Show the wordcloud\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding adjectives is a little more difficult, as they can be disbursed in the sentence.  But, there are rules that apply to adjectives that we can leverage.\n",
    "\n",
    "\n",
    "Which noun is in the sentence?\n",
    "Does the sentence end with another noun\n",
    "True until we hit another noun\n",
    "Get all of the adjectives\n",
    "First off, adjectives tend to appear in sentences in this order:\n",
    "1. Quantity or number\n",
    "2. Quality or opinion\n",
    "3. Size\n",
    "4. Age\n",
    "5. Shape\n",
    "6. Color\n",
    "7. Proper adjective (often nationality, other place of origin, or material)\n",
    "8. Purpose or qualifier\n",
    "\n",
    "If two or more adjectives are from the same group in the list above, the word \"and\" is placed between the two adjectives.  (e.g. \"The hall was decorated in red and green streamers.\")\n",
    "\n",
    "If there are three or more adjectives from the same group, there will be a comma between each of the adjectives. (e.g. \"The hall was decorated in red, white and green streamers.\" \n",
    "\n",
    "What does this mean for us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjectives(tagged_dict, p_list):\n",
    "    \n",
    "    # create a dictionary to store adjectives\n",
    "    descriptive_adj = {}\n",
    "    for p in p_list:\n",
    "        descriptive_adj[p] = []\n",
    "    \n",
    "    # loop through the tagged narrative pieces\n",
    "    for k, v in tagged_dict.items():\n",
    "        for sent in v:\n",
    "            if sent[-1][0] == k:\n",
    "                # case is that the final word in the phrase is the character's name, \"adjective noun\" e.g. \"clever Harry\"\n",
    "                if sent[-2][1] == \"JJ\":\n",
    "                    descriptive_adj[k].append(sent[-2][0])\n",
    "        \n",
    "            else:\n",
    "                for i in range(len(sent)):\n",
    "\n",
    "                    if sent[i][0] == k:\n",
    "                        # case where we have \"adjective noun\" e.g. \"brave Hermione\"\n",
    "                        try:\n",
    "                            if sent[i-1][1] == \"JJ\":\n",
    "                                descriptive_adj[k].append(sent[i-1][0])\n",
    "                        except:\n",
    "                            continue\n",
    "                        # case where we have \"noun verb adjective\" e.g. \"Hermione was brave\" or \n",
    "                        # \"Hermione was very brave\"\n",
    "                        try:\n",
    "                            if sent[i+1][1] == \"VBD\":\n",
    "                                if sent[i+2][1] == \"JJ\":\n",
    "                                    descriptive_adj[k].append(sent[i+2][0])\n",
    "                                elif sent[i+2][1] == \"RB\" and sent[i+3][1] == \"JJ\":\n",
    "                                    descriptive_adj[k].append(sent[i+3][0])\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "    return descriptive_adj\n",
    "\n",
    "\n",
    "descriptive_adj = adjectives(tagged_dict, p_list)\n",
    "print (descriptive_adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Isolating sexist language\n",
    "This part is tricky - sexism can be subtle or detectable only with context.  To keep this analysis straightforward, I assembled a list of sexist works to search for in the narrative.\n",
    "\n",
    "Because the Harry Potter series is written in English by a British writer, I focused on sources from the UK and countries in the Commonwealth.  Using this blog post by a [New Zealand blogger](http://sacraparental.com/2016/05/14/everyday-misogyny-122-subtly-sexist-words-women/) I had a first set of words and some excellent categories to begin with.  I found a number of [other](http://time.com/4268325/history-calling-women-shrill/) excellent articles about sexism in language, which I used to add to the `sexist_words` Python dictionary below, grouping them by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexist_words = { \n",
    "    'assertiveness': ['bossy', 'abrasive', 'ball-bust', 'aggressive', 'shrill', 'bolshy', 'intense', 'stroppy', 'mannish', 'strident', 'know-it-all'],\n",
    "    'behavior' : ['cackle', 'shriek', 'giggl', 'caterwaul', 'yowl', 'screech','gossip', 'dramatic', 'catty', 'bitch', 'nag', 'coldly', 'icy', 'shrew', 'humorless', 'man-hater', 'banshee', 'fishwife', 'lippy', 'ditzy', 'diva', 'prima donna', 'feisty', 'ladylike', 'bubbly', 'vivaious', 'flirt', 'sass', 'chatty', 'demure', 'modest', 'emotional', 'hysterical', 'hormonal', 'menstrual', 'flaky', 'over-sensitive'],\n",
    "    'sexuality': ['slut', 'trollop', 'frigid', 'tease', 'loose', 'man-eater', 'prude', 'curvy', 'cheap', 'frump', 'mouse', 'clotheshorse', 'hag'],\n",
    "    'relationship': ['spinster', 'barren', 'housewife', 'houseproud', 'mistress'],\n",
    "#     'praise': ['care', 'compassion', 'hard-working', 'conscientious', 'dependable', 'diligent', 'dedicated', 'tactful', 'interpersonal', 'warm', 'helpful'],\n",
    "}\n",
    "\n",
    "# making this into a list for easier analysis\n",
    "sexist_words_list = [v[j] for k, v in sexist_words.items() for j in range(len(v))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what words we are looking for, we want to make sure we get all versions of the words; if we only compare them to this list, we'd pick up on `cackle` but not `cackled`.   \n",
    "\n",
    "To to this, we'll use a stemming algorithm that's part of NLTK.  Let's look at some examples of how this works.  What we want to happen is to have all forms of a word have the same stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "\n",
    "# this works really well for some words\n",
    "print(\"Bossiness stem:\", st.stem('bossiness'))\n",
    "print(\"Bossily stem:\", st.stem('bossily'))\n",
    "print(\"Bossy stem\", st.stem('bossy'))\n",
    "\n",
    "print(\"Cackled stem:\", st.stem('cackled'))\n",
    "print(\"Cackling stem:\", st.stem('cackling'))\n",
    "print(\"Cackle stem:\", st.stem('cackle'))\n",
    "\n",
    "# but not for others\n",
    "print(\"Shreiking stem:\", st.stem('shrieking'))\n",
    "print(\"Shrieked stem:\", st.stem('shrieked'))\n",
    "print(\"Shriek stem:\", st.stem('shriek'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, this doesn't *always* work, so for extra insurance, we'll have a rule that if the root word is in a word, it counts. \n",
    "\n",
    "So, even though the stemmer isn't linking `shrieking` to `shreik`, looking for the letters in `shreik` would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sexist_words(t):\n",
    "    sexist_words_isolated = []\n",
    "    sexist_words_count = Counter()\n",
    "    for s in t: # (n, [()])\n",
    "        sent = pos_tag(s)\n",
    "        nouns = [j[0] for j in sent if 'NN' in j[1]]\n",
    "        for word in sent:\n",
    "            if st.stem(word[0]).lower() in sexist_words_list and word != \"Moody\":\n",
    "                sexist_words_isolated.append(word[0].lower())\n",
    "                sexist_words_count[word[0].lower()] +=1\n",
    "    return (sexist_words_isolated, sexist_words_count)\n",
    "\n",
    "p, count = sexist_words(narrative_split_sent)\n",
    "\n",
    "#sexist words\n",
    "print (\"*\" * 20, \"Sexist words\", \"*\" * 20)\n",
    "wc.generate(' '.join(p))\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Show the wordcloud\n",
    "plt.show()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1: 'icy': 4, 'screech': 3, 'bossy': 2, 'nagging': 2, 'shriek': 2, 'gossiped': 1, 'shrill': 1, 'cheap': 1, 'hags': 1, 'know-it-all': 1}\n",
    "2: Counter({'screech': 3, 'shrill': 3, 'hags': 2, 'icy': 2, 'haggis': 2, 'bossily': 1, 'icily': 1, 'shriek': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
